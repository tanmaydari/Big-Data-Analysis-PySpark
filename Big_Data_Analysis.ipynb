{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Big Data Analysis using PySpark"
      ],
      "metadata": {
        "id": "BIyzZHKDivOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Objective:\n",
        "\n",
        "Perform scalable data analysis using PySpark\n",
        "\n",
        "Extract insights from a large dataset\n",
        "\n",
        "Demonstrate Big Data processing techniques"
      ],
      "metadata": {
        "id": "Gp8sw7Fvi5C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Setup"
      ],
      "metadata": {
        "id": "rNv5Zk0zjBmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_6Mk6plZSt9",
        "outputId": "761c5842-a2da-4d8f-92fe-e5a5f59df0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Big Data Analysis Project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Started\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ID_biL4ZwO9",
        "outputId": "7a02babb-cd91-4eac-ed2b-fb097a0d78ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "XMCMt-zgjHmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/netflix_titles.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9QnEzykahRl",
        "outputId": "31418011-fbf3-47b5-c341-ea2f9e1d875c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- show_id: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- director: string (nullable = true)\n",
            " |-- cast: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- date_added: string (nullable = true)\n",
            " |-- release_year: string (nullable = true)\n",
            " |-- rating: string (nullable = true)\n",
            " |-- duration: string (nullable = true)\n",
            " |-- listed_in: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            "\n",
            "+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n",
            "|show_id|   type|               title|       director|                cast|      country|        date_added|release_year|rating| duration|           listed_in|         description|\n",
            "+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n",
            "|     s1|  Movie|Dick Johnson Is Dead|Kirsten Johnson|                NULL|United States|September 25, 2021|        2020| PG-13|   90 min|       Documentaries|As her father nea...|\n",
            "|     s2|TV Show|       Blood & Water|           NULL|Ama Qamata, Khosi...| South Africa|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|After crossing pa...|\n",
            "|     s3|TV Show|           Ganglands|Julien Leclercq|Sami Bouajila, Tr...|         NULL|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|To protect his fa...|\n",
            "|     s4|TV Show|Jailbirds New Orl...|           NULL|                NULL|         NULL|September 24, 2021|        2021| TV-MA| 1 Season|Docuseries, Reali...|Feuds, flirtation...|\n",
            "|     s5|TV Show|        Kota Factory|           NULL|Mayur More, Jiten...|        India|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|In a city of coac...|\n",
            "+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis Section"
      ],
      "metadata": {
        "id": "C5zMik3WjPri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total Records:\", df.count())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYHwht-xa240",
        "outputId": "35fc0740-ebef-40f1-9f85-106da8442629"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Records: 8809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"type\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xf90B81a-Kg",
        "outputId": "31f4be3a-001c-4675-8376-013c2524411d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         type|count|\n",
            "+-------------+-----+\n",
            "|         NULL|    1|\n",
            "|      TV Show| 2676|\n",
            "|        Movie| 6131|\n",
            "|William Wyler|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.groupBy(\"country\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm5RdARQbGZD",
        "outputId": "a17586c6-81b8-4574-9fa9-08d1a2c04153"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|       country|count|\n",
            "+--------------+-----+\n",
            "| United States| 2805|\n",
            "|         India|  972|\n",
            "|          NULL|  832|\n",
            "|United Kingdom|  419|\n",
            "|         Japan|  245|\n",
            "|   South Korea|  199|\n",
            "|        Canada|  181|\n",
            "|         Spain|  145|\n",
            "|        France|  123|\n",
            "|        Mexico|  110|\n",
            "+--------------+-----+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, year, trim, when, to_date\n",
        "\n",
        "# Step 1: Clean spaces\n",
        "df = df.withColumn(\"date_added_clean\", trim(col(\"date_added\")))\n",
        "\n",
        "# Step 2: Keep only rows that look like a date (contain a comma and numbers)\n",
        "df = df.withColumn(\n",
        "    \"date_added_clean\",\n",
        "    when(col(\"date_added_clean\").rlike(\"^[A-Za-z]+\\\\s\\\\d{1,2},\\\\s\\\\d{4}$\"),\n",
        "         col(\"date_added_clean\"))\n",
        ")\n",
        "\n",
        "# Step 3: Convert to date safely\n",
        "df = df.withColumn(\n",
        "    \"date_added_clean\",\n",
        "    to_date(col(\"date_added_clean\"), \"MMMM d, yyyy\")\n",
        ")\n",
        "\n",
        "# Step 4: Extract year\n",
        "df = df.withColumn(\"year_added\", year(col(\"date_added_clean\")))\n",
        "\n",
        "# Step 5: Remove invalid rows\n",
        "df_year = df.filter(col(\"year_added\").isNotNull())\n",
        "\n",
        "# Step 6: Year-wise count\n",
        "df_year.groupBy(\"year_added\") \\\n",
        "       .count() \\\n",
        "       .orderBy(\"year_added\") \\\n",
        "       .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9P38AwcbSO3",
        "outputId": "9f834f13-4a82-41d9-a268-b5fc5652c4a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|year_added|count|\n",
            "+----------+-----+\n",
            "|      2008|    2|\n",
            "|      2009|    2|\n",
            "|      2010|    1|\n",
            "|      2011|   13|\n",
            "|      2012|    3|\n",
            "|      2013|   11|\n",
            "|      2014|   24|\n",
            "|      2015|   81|\n",
            "|      2016|  429|\n",
            "|      2017| 1186|\n",
            "|      2018| 1647|\n",
            "|      2019| 2014|\n",
            "|      2020| 1873|\n",
            "|      2021| 1491|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"rating\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzNOcfaQclZw",
        "outputId": "bec5dc0e-8fc3-4443-ac8c-388138b80208"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|           rating|count|\n",
            "+-----------------+-----+\n",
            "|            TV-MA| 3195|\n",
            "|            TV-14| 2158|\n",
            "|            TV-PG|  862|\n",
            "|                R|  796|\n",
            "|            PG-13|  489|\n",
            "|            TV-Y7|  334|\n",
            "|             TV-Y|  307|\n",
            "|               PG|  286|\n",
            "|             TV-G|  220|\n",
            "|               NR|   80|\n",
            "|                G|   41|\n",
            "|             NULL|    6|\n",
            "|         TV-Y7-FV|    6|\n",
            "|               UR|    3|\n",
            "|            NC-17|    3|\n",
            "|             2021|    2|\n",
            "| November 1, 2020|    1|\n",
            "| Shavidee Trotter|    1|\n",
            "|    Adriane Lenox|    1|\n",
            "|    Maury Chaykin|    1|\n",
            "+-----------------+-----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scalability Section"
      ],
      "metadata": {
        "id": "Cv1Szjt3jdoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_repartitioned = df.repartition(4)\n",
        "print(\"Number of partitions:\", df_repartitioned.rdd.getNumPartitions())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJwKiGVFcqL1",
        "outputId": "5aad0d56-0b20-45cf-e655-02dbb9d72ea6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C-kH45dc6KQ",
        "outputId": "3f60e8dc-40bc-46e0-e586-90331d111879"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8809"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was repartitioned to enable parallel processing. Caching was applied to improve performance for repeated operations, demonstrating scalability using PySpark."
      ],
      "metadata": {
        "id": "dI9ONIxddYbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"type\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahlMM6qgdpip",
        "outputId": "8c1d2e3c-4f3c-453a-e385-dbb9baadde5b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         type|count|\n",
            "+-------------+-----+\n",
            "|         NULL|    1|\n",
            "|      TV Show| 2676|\n",
            "|        Movie| 6131|\n",
            "|William Wyler|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.groupBy(\"country\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgi4Rzh4dtJ1",
        "outputId": "4ec9817d-1cc7-41ad-f11c-132e15ff137b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----+\n",
            "|       country|count|\n",
            "+--------------+-----+\n",
            "| United States| 2805|\n",
            "|         India|  972|\n",
            "|          NULL|  832|\n",
            "|United Kingdom|  419|\n",
            "|         Japan|  245|\n",
            "|   South Korea|  199|\n",
            "|        Canada|  181|\n",
            "|         Spain|  145|\n",
            "|        France|  123|\n",
            "|        Mexico|  110|\n",
            "+--------------+-----+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"rating\") \\\n",
        "  .count() \\\n",
        "  .orderBy(col(\"count\").desc()) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcuIjHdbdwbW",
        "outputId": "faac3e9a-3c02-4298-a7aa-65415d964e55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|           rating|count|\n",
            "+-----------------+-----+\n",
            "|            TV-MA| 3195|\n",
            "|            TV-14| 2158|\n",
            "|            TV-PG|  862|\n",
            "|                R|  796|\n",
            "|            PG-13|  489|\n",
            "|            TV-Y7|  334|\n",
            "|             TV-Y|  307|\n",
            "|               PG|  286|\n",
            "|             TV-G|  220|\n",
            "|               NR|   80|\n",
            "|                G|   41|\n",
            "|             NULL|    6|\n",
            "|         TV-Y7-FV|    6|\n",
            "|               UR|    3|\n",
            "|            NC-17|    3|\n",
            "|             2021|    2|\n",
            "| November 1, 2020|    1|\n",
            "| Shavidee Trotter|    1|\n",
            "|    Adriane Lenox|    1|\n",
            "|    Maury Chaykin|    1|\n",
            "+-----------------+-----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "iChvG2uxd0Z6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Insights-\n",
        "\n",
        "Total records analyzed: ~8800+\n",
        "\n",
        "Movies dominate Netflix content\n",
        "\n",
        "USA produces the highest content\n",
        "\n",
        "Content growth increased after 2015\n",
        "\n",
        "TV-MA is the most common rating\n",
        "\n",
        "Data cleaning was performed to handle inconsistent values\n",
        "\n",
        "PySpark enabled distributed processing\n",
        "\n",
        "Scalability demonstrated using partitioning and caching\n",
        "\n",
        "Analysis executed on Google Colab cloud environment"
      ],
      "metadata": {
        "id": "MPhSbi_0eB0U"
      }
    }
  ]
}